# HyLIP
Predicting LLM Inference Latency with a Hybrid Modeling Approach


## Disclaimer
All data used in this project was collected via publicly available GPU profiling tools (e.g., Nsight, nvprof) on rented cloud GPU instances through platforms such as RunPod and Vast.ai. All profiling activities were conducted within the bounds of standard user access, on instances owned and rented by the author.

No attempts were made to reverse engineer, probe, or interfere with the underlying infrastructure of the service providers. The open-sourced models and data are for research and educational purposes only, and do not disclose any private or proprietary information of the platform providers.
